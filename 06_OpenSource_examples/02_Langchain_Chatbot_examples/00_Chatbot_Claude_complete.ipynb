{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Chatbot with Claude LLM\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "In this notebook, we will build a chatbot using the Foundation Models (FMs) in Amazon Bedrock. For our use-case we use Claude as our FM for building the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers.Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "\n",
    "## Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Chatbot (Basic)** - Zero Shot chatbot with a FM model\n",
    "2. **Chatbot using prompt** - template(Langchain) - Chatbot with some context provided in the prompt template\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "## Langchain framework for building Chatbot with Amazon Bedrock\n",
    "In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.\n",
    "\n",
    "## Building Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to **generate embeddings** for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using Titan Embeddings model for this\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)\n",
    "\n",
    "## Architecture [Context Aware Chatbot]\n",
    "![4](./images/context-aware-chatbot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../00_Intro/bedrock_boto3_setup.ipynb#Prerequisites) notebook. ⚠️ ⚠️ ⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.34.120)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.120 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.34.120)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.120->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.120->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.120->boto3) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain>=0.1.11 in /opt/conda/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (2.0.28)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: faiss-cpu<2,>=1.7 in /opt/conda/lib/python3.10/site-packages (1.8.0)\n",
      "Collecting pypdf<4,>=3.8\n",
      "  Downloading pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pinecone-client==2.2.4\n",
      "  Downloading pinecone_client-2.2.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting apache-beam==2.52.\n",
      "  Downloading apache_beam-2.52.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting tiktoken==0.5.2\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.10/site-packages (7.6.5)\n",
      "Collecting ipywidgets<8,>=7\n",
      "  Downloading ipywidgets-7.8.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting matplotlib==3.8.2\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting anthropic==0.9.0\n",
      "  Downloading anthropic-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (6.0.1)\n",
      "Collecting loguru>=0.5.0 (from pinecone-client==2.2.4)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (4.11.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (2.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (1.26.18)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (1.26.4)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (3.10.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.9.4)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.19)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.64.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.22.0)\n",
      "Requirement already satisfied: js2py<1,>=0.74 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.74)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (4.21.1)\n",
      "Collecting numpy>=1.22.0 (from pinecone-client==2.2.4)\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting objsize<0.7.0,>=0.6.1 (from apache-beam==2.52.)\n",
      "  Downloading objsize-0.6.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (23.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (4.7.3)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (4.25.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2024.1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2023.12.25)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.22.0)\n",
      "Collecting pyarrow<12.0.0,>=3.0.0 (from apache-beam==2.52.)\n",
      "  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (3.1.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (2.7.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (1.3.1)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic==0.9.0)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.2.4)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.1.73)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (8.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (3.0.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (0.2.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (5.14.2)\n",
      "Collecting widgetsnbextension~=3.6.6 (from ipywidgets<8,>=7)\n",
      "  Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (8.23.0)\n",
      "Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets<8,>=7)\n",
      "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic==0.9.0) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic==0.9.0) (1.2.0)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.52.) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.52.) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic==0.9.0) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic==0.9.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic==0.9.0) (0.14.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (4.9.0)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam==2.52.) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam==2.52.) (2.7.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.52.) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.52.) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.52.) (0.18.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain>=0.1.11) (1.33)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic==0.9.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->anthropic==0.9.0) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic==0.9.0) (0.23.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (6.5.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.9.0) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic==0.9.0) (2024.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8,>=7) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain>=0.1.11) (2.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (3.1.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (6.4)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (24.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (23.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (5.7.2)\n",
      "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (7.4.9)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (5.10.4)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (6.5.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.5.5)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (6.29.3)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.20.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8,>=7) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets<8,>=7) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7) (0.2.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client<8,>=5.3.4->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (4.1.0)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.14.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.2.4)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (4.12.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.19.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (21.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.8.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (5.9.8)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (7.7.0)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.5)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.21)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (20.11.0)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7) (2.9.0.20240316)\n",
      "Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading apache_beam-2.52.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m362.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anthropic-0.9.0-py3-none-any.whl (826 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.9/826.9 kB\u001b[0m \u001b[31m302.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m236.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m198.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m232.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m162.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m192.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m301.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sqlalchemy, pypdf, objsize, numpy, loguru, jupyterlab-widgets, tiktoken, pyarrow, pinecone-client, tokenizers, matplotlib, apache-beam, anthropic, widgetsnbextension, ipywidgets\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.28\n",
      "    Uninstalling SQLAlchemy-2.0.28:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.28\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 4.2.0\n",
      "    Uninstalling pypdf-4.2.0:\n",
      "      Successfully uninstalled pypdf-4.2.0\n",
      "  Attempting uninstall: objsize\n",
      "    Found existing installation: objsize 0.7.0\n",
      "    Uninstalling objsize-0.7.0:\n",
      "      Successfully uninstalled objsize-0.7.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: jupyterlab-widgets\n",
      "    Found existing installation: jupyterlab_widgets 3.0.10\n",
      "    Uninstalling jupyterlab_widgets-3.0.10:\n",
      "      Successfully uninstalled jupyterlab_widgets-3.0.10\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.6.0\n",
      "    Uninstalling tiktoken-0.6.0:\n",
      "      Successfully uninstalled tiktoken-0.6.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.4\n",
      "    Uninstalling matplotlib-3.8.4:\n",
      "      Successfully uninstalled matplotlib-3.8.4\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.56.0\n",
      "    Uninstalling apache-beam-2.56.0:\n",
      "      Successfully uninstalled apache-beam-2.56.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.5.2\n",
      "    Uninstalling widgetsnbextension-3.5.2:\n",
      "      Successfully uninstalled widgetsnbextension-3.5.2\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.6.5\n",
      "    Uninstalling ipywidgets-7.6.5:\n",
      "      Successfully uninstalled ipywidgets-7.6.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.19.2 requires pyarrow>=12.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.9.0 apache-beam-2.52.0 ipywidgets-7.8.1 jupyterlab-widgets-1.1.7 loguru-0.7.2 matplotlib-3.8.2 numpy-1.24.4 objsize-0.6.1 pinecone-client-2.2.4 pyarrow-11.0.0 pypdf-3.17.4 sqlalchemy-2.0.30 tiktoken-0.5.2 tokenizers-0.19.1 widgetsnbextension-3.6.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m343.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, transformers\n",
      "Successfully installed safetensors-0.4.3 transformers-4.41.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U --no-cache-dir boto3\n",
    "%pip install -U --no-cache-dir  \\\n",
    "    \"langchain>=0.1.11\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0\n",
    "%pip install -U --no-cache-dir transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot (Basic - without context)\n",
    "\n",
    "We use [CoversationChain](https://python.langchain.com/en/latest/modules/models/llms/integrations/bedrock.html?highlight=ConversationChain#using-in-a-conversation-chain) from LangChain to start the conversation. We also use the [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html) for storing the messages. We can also get the history as a list of messages (this is very useful in a chat model).\n",
    "\n",
    "Chatbots needs to remember the previous interactions. Conversational memory allows us to do that. There are several ways that we can implement conversational memory. In the context of LangChain, they are all built on top of the ConversationChain.\n",
    "\n",
    "**Note:** The model outputs are non-deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `Bedrock` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockLLM`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello! What would you like to chat about? I'm Claude, an AI assistant created by Anthropic to be\n",
      "helpful, harmless, and honest.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "modelId = \"anthropic.claude-v2\"\n",
    "cl_llm = Bedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 1000},\n",
    ")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=cl_llm, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "try:\n",
    "    \n",
    "    print_ww(conversation.predict(input=\"Hi there!\"))\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we used a basic prompt (\"Hi there!\") with no special formatting. In the next sections we will see how to enrich it\n",
    "with more detail.\n",
    "\n",
    "To learn more about how to write prompts for Claude, check [Anthropic documentation](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot using prompt template (Langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides several classes and functions to make constructing and working with prompts easy. We are going to use the [PromptTemplate](https://python.langchain.com/en/latest/modules/prompts/getting_started.html) class to construct the prompt from a f-string template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! Nice to meet you. I'm Claude, an AI assistant created by Anthropic to be helpful, harmless,\n",
      "and honest. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# turn verbose to true to see the full logs and documents\n",
    "conversation= ConversationChain(\n",
    "    llm=cl_llm, verbose=False, memory=ConversationBufferMemory() #memory_chain\n",
    ")\n",
    "\n",
    "# langchain prompts do not always work with all the models. This prompt is tuned for Claude\n",
    "claude_prompt = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "Human: The following is a friendly conversation between a human and an AI.\n",
    "The AI is talkative and provides lots of specific details from its context. If the AI does not know\n",
    "the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "<conversation_history>\n",
    "{history}\n",
    "</conversation_history>\n",
    "\n",
    "Here is the human's next reply:\n",
    "<human_reply>\n",
    "{input}\n",
    "</human_reply>\n",
    "\n",
    "Assistant:\n",
    "\"\"\")\n",
    "\n",
    "conversation.prompt = claude_prompt\n",
    "\n",
    "print_ww(conversation.predict(input=\"Hi there!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Questions\n",
    "\n",
    "Model has responded with intial message, let's ask few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are a few tips to help you start a new garden:\n",
      "\n",
      "- Decide what you want to grow - Think about what types of vegetables, herbs, or flowers you'd like\n",
      "to plant. Consider your space, sunlight, and climate when choosing. Some easy options for beginners\n",
      "are tomatoes, zucchini, bell peppers, lettuce, carrots, radishes, and marigolds.\n",
      "\n",
      "- Prepare the soil - Remove any grass or weeds where you plan to put the garden. Turn over and\n",
      "loosen the soil, mixing in compost or other organic material to enrich it. Good soil preparation\n",
      "helps plants thrive.\n",
      "\n",
      "- Make a plan - Sketch out a simple garden plan for where plants will go. Make sure taller plants\n",
      "don't block sun from shorter ones. Rotate plant families each year to prevent diseases.\n",
      "\n",
      "- Pick healthy starter plants - Look for stocky vegetable seedlings without yellowing leaves or\n",
      "flowers. For herbs and flowers, choose young plants with healthy, vigorous growth. Or start some\n",
      "seeds indoors for transplanting later.\n",
      "\n",
      "- Plant at the right time - Follow planting guidelines for your climate to see when to sow seeds or\n",
      "transplant starters outside. Spring and fall are common garden planting times.\n",
      "\n",
      "- Water thoroughly at planting - Ensure plants get plenty of water right after planting to promote\n",
      "good root growth. Add mulch around plants to retain moisture.\n",
      "\n",
      "- Provide ongoing care - Weed, water, and fertilize as needed through the season. Stake up tall\n",
      "plants for support. Watch for pests and diseases. Enjoy the fruits (and flowers) of your labor!\n",
      "\n",
      "Let me know if you need any other gardening tips! I'm happy to share more specific advice once you\n",
      "decide what you want to grow.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"Give me a few tips on how to start a new garden.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build on the questions\n",
    "\n",
    "Let's ask a question without mentioning the word garden to see if model can understand previous conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, growing tomatoes would work well with the gardening tips I provided earlier. Here are some\n",
      "more specific tomato growing tips:\n",
      "\n",
      "- Choose disease-resistant tomato varieties - Look for varieties labeled VFN or VFNT as these are\n",
      "resistant to common tomato diseases like verticillium, fusarium, nematodes, and tobacco mosaic\n",
      "virus. Some good options are Celebrity, Better Boy, Big Beef.\n",
      "\n",
      "- Prepare the soil - Tomatoes thrive in fertile, well-draining soil. Mix in compost or rotted manure\n",
      "before planting. They prefer slightly acidic soil with a pH between 6.0-6.8.\n",
      "\n",
      "- Plant seedlings 1-2 feet apart - Give tomato plants plenty of space to allow air circulation and\n",
      "prevent disease. Plant them deep by burying much of the stem.\n",
      "\n",
      "- Use cages or stakes - Support tomatoes with tomato cages or staking to keep the fruit and foliage\n",
      "off the ground. This prevents disease and makes harvesting easier.\n",
      "\n",
      "- Water at the base - Water deeply 1-2 inches per week. Avoid wetting leaves which can promote\n",
      "diseases. Use drip irrigation or soaker hoses.\n",
      "\n",
      "- Prune suckers & leaves - Prune off suckers (shoots between main stem & branches) to keep plants\n",
      "tidy and productive. Remove lower leaves to help air circulate.\n",
      "\n",
      "- Monitor for pests & disease - Watch for common tomato pests like hornworms, cutworms, or aphids.\n",
      "Remove damaged leaves or spots of disease promptly.\n",
      "\n",
      "Let me know if you have any other questions! Tomatoes are a very rewarding crop for home gardeners.\n",
      "With good care and maintenance, you can get a bountiful harvest.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"Cool. Will that work with tomatoes?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finishing this conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're very welcome! I'm glad I could provide some helpful tomato growing tips to get your new\n",
      "garden started off right. It sounds like you've got a good plan in place. Wishing you the best of\n",
      "luck with your gardening and a wonderful tomato harvest this season! Feel free to reach out if any\n",
      "other questions come up as you begin prepping your garden and caring for your tomato plants. I'm\n",
      "always happy to offer more advice. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"That's all, thank you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude is still really talkative. Try changing the prompt to make Claude provide shorter answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello!\n"
     ]
    }
   ],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "conversation2= ConversationChain(\n",
    "    llm=cl_llm, verbose=False, memory=ConversationBufferMemory() #memory_chain\n",
    ")\n",
    "\n",
    "# langchain prompts do not always work with all the models. This prompt is tuned for Claude\n",
    "claude_prompt2 = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "Human: The following is a  conversation between a human and an AI.\n",
    "The AI is not talkative and provides succint answers only. The AI does not elaborate unless asked. If the AI does not know\n",
    "the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "<conversation_history>\n",
    "{history}\n",
    "</conversation_history>\n",
    "\n",
    "Here is the human's next reply:\n",
    "<human_reply>\n",
    "{input}\n",
    "</human_reply>\n",
    "\n",
    "Assistant:\n",
    "\"\"\")\n",
    "\n",
    "conversation2.prompt = claude_prompt2\n",
    "\n",
    "print_ww(conversation2.predict(input=\"Hi there!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are a few tips for starting a new garden:\n",
      "\n",
      "- Decide what you want to grow - vegetables, flowers, herbs? Select plants that are suitable for\n",
      "your climate and space.\n",
      "\n",
      "- Prepare the soil - remove weeds, mix in compost or fertilizer to enrich the soil. You want nice\n",
      "loose soil for plant roots.\n",
      "\n",
      "- Make a plan - sketch out garden beds and paths so you know what to plant where. Consider sunlight\n",
      "needs for different plants.\n",
      "\n",
      "- Start seeds indoors - begin seeds like tomatoes and peppers a few weeks before last frost date.\n",
      "Harden off plants before transplanting.\n",
      "\n",
      "- Plant hardy crops first - Cool weather crops like spinach, lettuce, onions can be planted early in\n",
      "soil. Hold off on warm weather plants.\n",
      "\n",
      "- Water new plants well - Don't let seedlings dry out as they establish roots. Provide 1-2 inches of\n",
      "water per week for most vegetables.\n",
      "\n",
      "- Add mulch - Place mulch like wood chips or straw around plants to retain moisture and discourage\n",
      "weeds.\n",
      "\n",
      "- Be patient! It takes time for a new garden to flourish. Pay attention and make adjustments as you\n",
      "learn.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation2.predict(input=\"Give me a few tips on how to start a new garden.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, the tips I provided will work well for growing tomatoes in a new garden. Here are some key\n",
      "points for tomatoes specifically:\n",
      "\n",
      "- Determine if you want bush or vining tomato varieties. Vining types will need cages or trellises\n",
      "for support.\n",
      "\n",
      "- Start tomato seeds indoors 6-8 weeks before your last expected spring frost. Tomatoes need time to\n",
      "establish before going outside.\n",
      "\n",
      "- Transplant tomatoes when soil is warm, after hardening off the seedlings. Wait until all danger of\n",
      "frost has passed.\n",
      "\n",
      "- Plant tomatoes deeply, burying much of the stem. This encourages more root growth. Space plants\n",
      "18-24 inches apart.\n",
      "\n",
      "- Use tomato cages or staking to support vines as the plants grow. This prevents fallen and rotting\n",
      "fruit.\n",
      "\n",
      "- Water at the base of plants, avoiding wet leaves. Provide 1-2 inches of water per week.\n",
      "\n",
      "- Mulch around tomatoes heavily. This retains moisture and reduces weeds.\n",
      "\n",
      "- Pinch off suckers (branches between main stem and leaves) to focus energy on fruiting.\n",
      "\n",
      "Let me know if you need any other tomato growing tips!\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation2.predict(input=\"Cool. Will that work with tomatoes?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're very welcome! I'm glad I could provide some helpful tips for starting a new vegetable\n",
      "garden, especially for growing tomatoes. Best of luck with your gardening this season! Feel free to\n",
      "reach out if you have any other questions as you get your new garden going.\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation2.predict(input=\"That's all, thank you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive session using ipywidgets\n",
    "\n",
    "The following utility class allows us to interact with Claude in a more natural way. We write out the question in an input box, and get Claude's answer. We can then continue our conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat!!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        result = self.qa.run({'question': prompt })\n",
    "                    else:\n",
    "                        result = self.qa.run({'input': prompt }) #, 'history':chat_history})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print_ww(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start a chat. You can also test the following questions:\n",
    "1. tell me a joke\n",
    "2. tell me another joke\n",
    "3. what was the first joke about\n",
    "4. can you make another joke on the same topic of the first joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896453210bb245ebb31fbad267189240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(conversation)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot with persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI assistant will play the role of a career coach. Role Play Dialogue requires user message to be set in before starting the chat. ConversationBufferMemory is used to pre-populate the dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Human: The following is a friendly conversation between a human and an AI.\n",
      "The AI is talkative and provides lots of specific details from its context. If the AI does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "<conversation_history>\n",
      "Human: You will be acting as a career coach. Your goal is to give career advice to users\n",
      "AI: I am a career coach and give career advice\n",
      "</conversation_history>\n",
      "\n",
      "Here is the human's next reply:\n",
      "<human_reply>\n",
      "What are the career options in AI?\n",
      "</human_reply>\n",
      "\n",
      "Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Here are some of the top career options in AI (artificial intelligence):\n",
      "\n",
      "- Machine Learning Engineer - These engineers develop and implement machine learning algorithms and\n",
      "models to enable systems and software to improve and learn over time. Skills needed include\n",
      "programming languages like Python and familiarity with ML frameworks like TensorFlow.\n",
      "\n",
      "- Data Scientist - Data scientists analyze and interpret complex data to uncover insights, trends,\n",
      "and opportunities for improvement. Skills include data mining, statistical modeling, machine\n",
      "learning, and programming languages like R and Python.\n",
      "\n",
      "- AI Researcher - AI researchers push the boundaries of what's possible in artificial intelligence.\n",
      "They invent new machine learning methods, design experiments, and publish academic papers. Requires\n",
      "a PhD in computer science or a related field.\n",
      "\n",
      "- AI Software Engineer - AI software engineers build the platforms and infrastructure to deploy AI\n",
      "systems at scale. Skills include software engineering, cloud computing, and coding languages like\n",
      "Python, Java, C++.\n",
      "\n",
      "- Robotics Engineer - Robotics engineers design, program and test robots and autonomous systems.\n",
      "Skills needed are mechanics, electronics, software engineering and computer simulation.\n",
      "\n",
      "- Computer Vision Engineer - Computer vision engineers develop algorithms and systems for\n",
      "processing, analyzing and understanding visual data from digital images and videos. Skills include\n",
      "machine learning, computer vision, and coding languages like Python.\n",
      "\n",
      "- Natural Language Processing Engineer - NLP engineers build systems that can understand, interpret\n",
      "and generate human language. Skills include linguistics, machine learning (especially deep\n",
      "learning), and programming languages like Python and Java.\n",
      "\n",
      "Let me know if you need any advice on pursuing a career in AI or have additional questions!\n"
     ]
    }
   ],
   "source": [
    "# store previous interactions using ConversationalBufferMemory and add custom prompts to the chat.\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"You will be acting as a career coach. Your goal is to give career advice to users\")\n",
    "memory.chat_memory.add_ai_message(\"I am a career coach and give career advice\")\n",
    "cl_llm = Bedrock(model_id=\"anthropic.claude-v2\",client=boto3_bedrock)\n",
    "conversation = ConversationChain(\n",
    "     llm=cl_llm, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "conversation.prompt = claude_prompt\n",
    "\n",
    "print_ww(conversation.predict(input=\"What are the career options in AI?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Human: The following is a friendly conversation between a human and an AI.\n",
      "The AI is talkative and provides lots of specific details from its context. If the AI does not know\n",
      "the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "<conversation_history>\n",
      "Human: You will be acting as a career coach. Your goal is to give career advice to users\n",
      "AI: I am a career coach and give career advice\n",
      "Human: What are the career options in AI?\n",
      "AI:  Here are some of the top career options in AI (artificial intelligence):\n",
      "\n",
      "- Machine Learning Engineer - These engineers develop and implement machine learning algorithms and models to enable systems and software to improve and learn over time. Skills needed include programming languages like Python and familiarity with ML frameworks like TensorFlow.\n",
      "\n",
      "- Data Scientist - Data scientists analyze and interpret complex data to uncover insights, trends, and opportunities for improvement. Skills include data mining, statistical modeling, machine learning, and programming languages like R and Python. \n",
      "\n",
      "- AI Researcher - AI researchers push the boundaries of what's possible in artificial intelligence. They invent new machine learning methods, design experiments, and publish academic papers. Requires a PhD in computer science or a related field.\n",
      "\n",
      "- AI Software Engineer - AI software engineers build the platforms and infrastructure to deploy AI systems at scale. Skills include software engineering, cloud computing, and coding languages like Python, Java, C++.\n",
      "\n",
      "- Robotics Engineer - Robotics engineers design, program and test robots and autonomous systems. Skills needed are mechanics, electronics, software engineering and computer simulation. \n",
      "\n",
      "- Computer Vision Engineer - Computer vision engineers develop algorithms and systems for processing, analyzing and understanding visual data from digital images and videos. Skills include machine learning, computer vision, and coding languages like Python.\n",
      "\n",
      "- Natural Language Processing Engineer - NLP engineers build systems that can understand, interpret and generate human language. Skills include linguistics, machine learning (especially deep learning), and programming languages like Python and Java.\n",
      "\n",
      "Let me know if you need any advice on pursuing a career in AI or have additional questions!\n",
      "</conversation_history>\n",
      "\n",
      "Here is the human's next reply:\n",
      "<human_reply>\n",
      "What these people really do? Is it fun?\n",
      "</human_reply>\n",
      "\n",
      "Assistant:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Here are some more details on what these AI careers entail and whether they are enjoyable:\n",
      "\n",
      "Machine Learning Engineers:\n",
      "- Build, optimize and deploy ML models into production applications. Work can involve data cleaning,\n",
      "feature engineering, testing models, and monitoring performance.\n",
      "- Get to see machine learning algorithms come to life and impact real products. Fun creative problem\n",
      "solving.\n",
      "\n",
      "Data Scientists:\n",
      "- Spend time discovering trends and patterns in data. Use statistics, machine learning and\n",
      "programming to extract meaning from complex datasets.\n",
      "- Enjoy the \"aha!\" moments when they uncover valuable insights to inform business strategy. Fun mix\n",
      "of analytics and storytelling.\n",
      "\n",
      "AI Researchers:\n",
      "- Conduct studies and experiments to create new ML methods or advance a research area. Involves lots\n",
      "of coding, theory, writing papers.\n",
      "- Work on cutting edge innovations. Rewarding to push boundaries of the field, though can be\n",
      "solitary.\n",
      "\n",
      "AI Software Engineers:\n",
      "- Design and code AI applications, platforms and infrastructure. Integrate ML models into products.\n",
      "- Satisfying to build useful AI-powered products and bring abstract algorithms to life.\n",
      "\n",
      "Robotics Engineers:\n",
      "- Design, program, simulate and test robotic systems and devices. Combines both software and\n",
      "hardware.\n",
      "- Rewarding to see your robot designs come to life. Fun if you enjoy tinkering with electronics and\n",
      "coding.\n",
      "\n",
      "Computer Vision Engineers:\n",
      "- Develop algorithms to enable computers to identify, process and analyze visual data from digital\n",
      "images and videos.\n",
      "- Exciting to work on systems that can autonomously understand real world environments and objects.\n",
      "\n",
      "NLP Engineers:\n",
      "- Build systems that understand, interpret, and generate human languages using machine learning.\n",
      "- Interesting to see computers complete human-like language tasks. Challenging work focused on\n",
      "modeling human language.\n",
      "\n",
      "Overall, most AI careers involve a good mix of computer science, math, and specialized knowledge.\n",
      "They appeal to people who enjoy coding, problem-solving, and seeing their work impact real products\n",
      "and systems. Let me know if you have any other questions!\n"
     ]
    }
   ],
   "source": [
    "print_ww(conversation.predict(input=\"What these people really do? Is it fun?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's ask a question that is not specialty of this Persona and the model shouldn't answer that question and give a reason for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I apologize, but I do not have enough context to provide helpful advice on how to fix your car.\n",
      "Cars can have many different issues, from mechanical problems to electrical faults, and the solution\n",
      "would depend on the specific make, model and symptoms you are experiencing. Without more details on\n",
      "what problem you are facing, I would not want to guess or provide generalized car repair suggestions\n",
      "that may not apply. I'd recommend consulting a mechanic who can properly diagnose and address the\n",
      "issues with your car. Let me know if you have any other questions I may be better equipped to\n",
      "answer!\n"
     ]
    }
   ],
   "source": [
    "conversation.verbose = False\n",
    "print_ww(conversation.predict(input=\"How do I fix my car?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with Context \n",
    "In this use case we will ask the Chatbot to answer question from some external corpus it has likely never seen before. To do this we apply a pattern called RAG (Retrieval Augmented Generation): the idea is to index the corpus in chunks, then look up which sections of the corpus might be relevant to provide an answer by using semantic similarity between the chunks and the question. Finally the most relevant chunks are aggregated and passed as context to the ConversationChain, similar to providing a history.\n",
    "\n",
    "We will take a csv file and use **Titan Embeddings Model** to create vectors for each line of the csv. This vector is then stored in FAISS, an open source library providing an in-memory vector datastore. When the chatbot is asked a question, we query FAISS with the question and retrieve the text which is semantically closest. This will be our answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titan embeddings Model\n",
    "\n",
    "Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "Embeddings are for example used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS as VectorStore\n",
    "\n",
    "In order to be able to use embeddings for search, we need a store that can efficiently perform vector similarity searches. In this notebook we use FAISS, which is an in memory store. For permanently store vectors, one can use pgVector, Pinecone or Chroma.\n",
    "\n",
    "The langchain VectorStore API's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "To know more about the FAISS vector store please refer to this [document](https://arxiv.org/pdf/1702.08734.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to rag_data/Amazon_SageMaker_FAQs.csv\n",
      "Number of documents=153\n",
      "Number of documents after split and chunking=154\n",
      "vectorstore_faiss_aws: number of elements in the index=154::\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "s3_path = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv\"\n",
    "!aws s3 cp $s3_path ./rag_data/Amazon_SageMaker_FAQs.csv\n",
    "\n",
    "loader = CSVLoader(\"./rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "documents_aws = loader.load() #\n",
    "print(f\"Number of documents={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic search\n",
    "\n",
    "We can use a Wrapper class provided by LangChain to query the vector data base store and return to us the relevant documents. Behind the scenes this is only going to run a RetrievalQA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, here are some key points about using R with Amazon SageMaker:\n",
      "\n",
      "- R is supported in SageMaker through RStudio Workbench and SageMaker Studio notebooks with a\n",
      "preinstalled R kernel. This allows you to develop R models and scripts and integrate them into the\n",
      "SageMaker ML workflow.\n",
      "\n",
      "- Reticulate provides an R interface to the SageMaker Python SDK, enabling you to build, train,\n",
      "tune, and deploy R models using the SageMaker infrastructure and capabilities.\n",
      "\n",
      "- SageMaker Studio provides a unified visual interface to work with RStudio IDE and SageMaker\n",
      "notebooks seamlessly, with automatic syncing of artifacts between environments.\n",
      "\n",
      "- You can leverage SageMaker Experiments to track and compare R modeling experiments and iterations\n",
      "in the Studio UI.\n",
      "\n",
      "- Overall, R is a fully supported language in SageMaker with capabilities to develop, experiment,\n",
      "and deploy R ML models at scale using SageMaker's managed infrastructure and tools.\n"
     ]
    }
   ],
   "source": [
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print_ww(wrapper_store_faiss.query(\"R in SageMaker\", llm=cl_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.171875, 0.33398438, 0.3125, -0.24316406, 0.60546875, 0.41992188, -0.36132812, -6.580353e-05, 0.3203125, -0.66796875]\n",
      "﻿What is Amazon SageMaker?: Is R supported with Amazon SageMaker?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Yes,\n",
      "R is supported with Amazon SageMaker. You can use R within SageMaker notebook instances, which\n",
      "include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the\n",
      "Amazon SageMaker Python SDK, enabling ML practitioners to build, train, tune, and deploy R models.\n",
      "----\n",
      "﻿What is Amazon SageMaker?: What is RStudio on Amazon SageMaker?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.:\n",
      "RStudio on Amazon SageMaker is the first fully managed RStudio Workbench in the cloud. You can\n",
      "quickly launch the familiar RStudio integrated development environment (IDE) and dial up and down\n",
      "the underlying compute resources without interrupting your work, making it easy to build machine\n",
      "learning (ML) and analytics solutions in R at scale. You can seamlessly switch between the RStudio\n",
      "IDE and Amazon SageMaker Studio notebooks for R and Python development. All your work, including\n",
      "code, datasets, repositories, and other artifacts, is automatically synchronized between the two\n",
      "environments to reduce context switch and boost productivity.\n",
      "----\n",
      "﻿What is Amazon SageMaker?: What is Amazon SageMaker Studio?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.:\n",
      "Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML\n",
      "development steps. SageMaker Studio gives you complete access, control, and visibility into each\n",
      "step required to prepare data and build, train, and deploy models. You can quickly upload data,\n",
      "create new notebooks, train and tune models, move back and forth between steps to adjust\n",
      "experiments, compare results, and deploy models to production all in one place, making you much more\n",
      "productive. All ML development activities including notebooks, experiment management, automatic\n",
      "model creation, debugging and profiling, and model drift detection can be performed within the\n",
      "unified SageMaker Studio visual interface.\n",
      "----\n",
      "﻿What is Amazon SageMaker?: What is Amazon SageMaker Experiments?\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine\n",
      "learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.:\n",
      "Amazon SageMaker Experiments helps you organize and track iterations to ML models. SageMaker\n",
      "Experiments helps you manage iterations by automatically capturing the input parameters,\n",
      "configurations, and results, and storing them as \"experiments\". You can work within the visual\n",
      "interface of Amazon SageMaker Studio, where you can browse active experiments, search for previous\n",
      "experiments by their characteristics, review previous experiments with their results, and compare\n",
      "experiment results visually.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "v = br_embeddings.embed_query(\"R in SageMaker\")\n",
    "print(v[0:10])\n",
    "results = vectorstore_faiss_aws.similarity_search_by_vector(v, k=4)\n",
    "for r in results:\n",
    "    print_ww(r.page_content)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "In any chatbot we will need a QA Chain with various options which are customized by the use case. But in a chatbot we will always need to keep the history of the conversation so the model can take it into consideration to provide the answer. In this example we use the [ConversationalRetrievalChain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db) from LangChain, together with a ConversationBufferMemory to keep the history of the conversation.\n",
    "\n",
    "Source: https://python.langchain.com/docs/modules/chains/popular/chat_vector_db\n",
    "\n",
    "Set `verbose` to `True` to see all the what is going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following conversation and a follow up question, rephrase the follow up question to be a\n",
      "standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "{chat_history}\n",
      "Follow Up Input: {question}\n",
      "Standalone question:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "print_ww(CONDENSE_QUESTION_PROMPT.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters used for ConversationRetrievalChain\n",
    "* **retriever**: We used `VectorStoreRetriever`, which is backed by a `VectorStore`. To retrieve text, there are two search types you can choose: `\"similarity\"` or `\"mmr\"`. `search_type=\"similarity\"` uses similarity search in the retriever object where it selects text chunk vectors that are most similar to the question vector.\n",
    "\n",
    "* **memory**: Memory Chain to store the history \n",
    "\n",
    "* **condense_question_prompt**: Given a question from the user, we use the previous conversation and that question to make up a standalone question\n",
    "\n",
    "* **chain_type**: If the chat history is long and doesn't fit the context you use this parameter and the options are `stuff`, `refine`, `map_reduce`, `map-rerank`\n",
    "\n",
    "If the question asked is outside the scope of context, then the model will reply it doesn't know the answer\n",
    "\n",
    "**Note**: if you are curious how the chain works, uncomment the `verbose=True` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cl_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    memory=memory_chain,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    #verbose=True, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chat! ask the chatbot some questions about SageMaker, like:\n",
    "1. What is SageMaker?\n",
    "2. What is canvas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7165ac0d726e460cb31240fdb6d52132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your mileage might vary, but after 2 or 3 questions you will start to get some weird answers. In some cases, even in other languages.\n",
    "This is happening for the same reasons outlined at the beginning of this notebook: the default langchain prompts are not optimal for Claude. \n",
    "In the following cell we are going to set two new prompts: one for the question rephrasing, and one to get the answer from that rephrased question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn verbose to true to see the full logs and documents\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema import BaseMessage\n",
    "\n",
    "\n",
    "# We are also providing a different chat history retriever which outputs the history as a Claude chat (ie including the \\n\\n)\n",
    "_ROLE_MAP = {\"human\": \"\\n\\nHuman: \", \"ai\": \"\\n\\nAssistant: \"}\n",
    "def _get_chat_history(chat_history):\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        if isinstance(dialogue_turn, BaseMessage):\n",
    "            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f\"{dialogue_turn.type}: \")\n",
    "            buffer += f\"\\n{role_prefix}{dialogue_turn.content}\"\n",
    "        elif isinstance(dialogue_turn, tuple):\n",
    "            human = \"\\n\\nHuman: \" + dialogue_turn[0]\n",
    "            ai = \"\\n\\nAssistant: \" + dialogue_turn[1]\n",
    "            buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported chat history format: {type(dialogue_turn)}.\"\n",
    "                f\" Full chat history: {chat_history} \"\n",
    "            )\n",
    "    return buffer\n",
    "\n",
    "# the condense prompt for Claude\n",
    "condense_prompt_claude = PromptTemplate.from_template(\"\"\"{chat_history}\n",
    "\n",
    "Answer only with the new question.\n",
    "\n",
    "\n",
    "Human: How would you ask the question considering the previous conversation: {question}\n",
    "\n",
    "\n",
    "Assistant: Question:\"\"\")\n",
    "\n",
    "# recreate the Claude LLM with more tokens to sample - this provides longer responses but introduces some latency\n",
    "cl_llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 500})\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cl_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 8}),\n",
    "    memory=memory_chain,\n",
    "    get_chat_history=_get_chat_history,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=condense_prompt_claude, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")\n",
    "\n",
    "# the LLMChain prompt to get the answer. the ConversationalRetrievalChange does not expose this parameter in the constructor\n",
    "qa.combine_docs_chain.llm_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "{context}\n",
    "\n",
    "Human: Use at maximum 3 sentences to answer the question inside the <q></q> XML tags. \n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "Do not use any XML tags in the answer. If the answer is not in the context say \"Sorry, I don't know as the answer was not found in the context\"\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start another chat. Feel free to ask the following questions:\n",
    "\n",
    "1. What is SageMaker?\n",
    "2. what is canvas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ca75773622465abde6e02dce20b4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some prompt engineering\n",
    "\n",
    "You can \"tune\" your prompt to get more or less verbose answers. For example, try to change the number of sentences, or remove that instruction all-together. You might also need to change the number of `max_tokens_to_sample` (eg 1000 or 2000) to get the full answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are also providing a different chat history retriever which outputs the history as a Claude chat (ie including the \\n\\n)\n",
    "_ROLE_MAP = {\"human\": \"\\n\\nHuman: \", \"ai\": \"\\n\\nAssistant: \"}\n",
    "def _get_chat_history(chat_history):\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        if isinstance(dialogue_turn, BaseMessage):\n",
    "            role_prefix = _ROLE_MAP.get(dialogue_turn.type, f\"{dialogue_turn.type}: \")\n",
    "            buffer += f\"\\n{role_prefix}{dialogue_turn.content}\"\n",
    "        elif isinstance(dialogue_turn, tuple):\n",
    "            human = \"\\n\\nHuman: \" + dialogue_turn[0]\n",
    "            ai = \"\\n\\nAssistant: \" + dialogue_turn[1]\n",
    "            buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported chat history format: {type(dialogue_turn)}.\"\n",
    "                f\" Full chat history: {chat_history} \"\n",
    "            )\n",
    "    return buffer\n",
    "\n",
    "# the condense prompt for Claude\n",
    "condense_prompt_claude = PromptTemplate.from_template(\"\"\"{chat_history}\n",
    "\n",
    "Answer only with the new question.\n",
    "\n",
    "\n",
    "Human: How would you ask the question considering the previous conversation: {question}\n",
    "\n",
    "\n",
    "Assistant: Question:\"\"\")\n",
    "\n",
    "# recreate the Claude LLM with more tokens to sample - this provides longer responses but introduces some latency\n",
    "cl_llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 2000})\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=cl_llm, \n",
    "    retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(search_type='similarity', search_kwargs={\"k\": 8}),\n",
    "    memory=memory_chain,\n",
    "    get_chat_history=_get_chat_history,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=condense_prompt_claude, \n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")\n",
    "\n",
    "# the LLMChain prompt to get the answer. the ConversationalRetrievalChange does not expose this parameter in the constructor\n",
    "qa.combine_docs_chain.llm_chain.prompt = PromptTemplate.from_template(\"\"\"\n",
    "{context}\n",
    "\n",
    "Human: Provide a lengthy and detailed answer for the question inside the <q></q> XML tags. \n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "Do not use any XML tags in the answer. If the answer is not in the context say \"Sorry, I don't know as the answer was not found in the context\"\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeb04ce74aa4f20823726db1001e419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatUX(qa, retrievalChain=True)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this demo we used Claude LLM to create conversational interface with following patterns:\n",
    "\n",
    "1. Chatbot (Basic - without context)\n",
    "\n",
    "2. Chatbot using prompt template(Langchain)\n",
    "\n",
    "3. Chatbot with personas\n",
    "\n",
    "4. Chatbot with context"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
